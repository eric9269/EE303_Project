#!/usr/bin/env python3
"""
ç°¡å–®ä½†æ­£ç¢ºçš„ MySQL æª”æ¡ˆè§£æå™¨
åŸºæ–¼çœŸå¯¦çš„æ¬„ä½çµæ§‹åˆ†æ
"""

import pandas as pd
from pathlib import Path
import re
import json

def parse_real_structure():
    """è§£æçœŸå¯¦çš„æ¬„ä½çµæ§‹"""
    print("è§£æçœŸå¯¦çš„ MySQL è¡¨æ ¼çµæ§‹")
    print("=" * 80)
    
    # åŸºæ–¼æˆ‘å€‘ä¹‹å‰çš„åˆ†æï¼ŒçœŸæ­£çš„æ¬„ä½çµæ§‹æ‡‰è©²æ˜¯ï¼š
    # 1. product_id - ç”¢å“è­˜åˆ¥ç¢¼
    # 2. image_urls - åœ–ç‰‡URLåˆ—è¡¨
    # 3. product_page_url - ç”¢å“é é¢URL
    # 4. price_info - åƒ¹æ ¼è³‡è¨Š
    # 5. shop_info - å•†åº—è³‡è¨Š
    # 6. category_info - åˆ†é¡è³‡è¨Š
    # 7. metadata - å…¶ä»–å…ƒè³‡æ–™
    
    # å¾æˆ‘å€‘ä¹‹å‰æå–çš„è³‡æ–™ä¸­é‡æ–°çµ„ç¹”
    data_dir = Path("../src/product_matching/extracted_data")
    
    if not data_dir.exists():
        print("âŒ extracted_data ç›®éŒ„ä¸å­˜åœ¨")
        print("è«‹ç¢ºä¿ src/product_matching/extracted_data ç›®éŒ„å­˜åœ¨")
        return
    
    # é¸æ“‡ä»£è¡¨æ€§çš„è¡¨æ ¼
    representative_files = [
        ("test_data.csv", "Test Data"),
        ("pm_leaf_v1_extracted.csv", "LEAF v1"),
        ("pm_root_v1_extracted.csv", "ROOT v1"),
        ("pm_leaf_it_shopee_v1_extracted.csv", "LEAF IT Shopee"),
        ("pm_root_it_shopee_v1_extracted.csv", "ROOT IT Shopee")
    ]
    
    all_results = {}
    
    for file_name, table_name in representative_files:
        file_path = data_dir / file_name
        
        if file_path.exists():
            print(f"\nğŸ“„ é‡æ–°çµ„ç¹” {file_name}")
            print("-" * 40)
            
            # é‡æ–°çµ„ç¹”è³‡æ–™
            reorganized_data = reorganize_data(str(file_path), table_name)
            
            if reorganized_data:
                all_results[table_name] = reorganized_data
    
    # ä¿å­˜é‡æ–°çµ„ç¹”çš„è³‡æ–™
    save_reorganized_data(all_results)
    
    return all_results

def reorganize_data(file_path, table_name):
    """é‡æ–°çµ„ç¹”è³‡æ–™ç‚ºæ­£ç¢ºçš„æ¬„ä½çµæ§‹"""
    try:
        df = pd.read_csv(file_path, encoding='utf-8-sig')
        
        print(f"åŸå§‹è³‡æ–™: {len(df.columns)} æ¬„ä½ x {len(df)} è¡Œ")
        
        # é‡æ–°çµ„ç¹”è³‡æ–™
        reorganized_rows = []
        
        for i in range(min(50, len(df))):  # åªè™•ç†å‰50è¡Œ
            row_data = df.iloc[i]
            
            # æ”¶é›†æ‰€æœ‰éç©ºè³‡æ–™
            all_data = []
            for col_idx, value in enumerate(row_data):
                if pd.notna(value) and str(value).strip():
                    all_data.append(str(value))
            
            # åˆ†æè³‡æ–™çµæ§‹
            row_analysis = analyze_row_data(all_data, i)
            
            if row_analysis:
                reorganized_rows.append(row_analysis)
        
        print(f"é‡æ–°çµ„ç¹”: {len(reorganized_rows)} è¡Œ")
        
        # çµ±è¨ˆåˆ†æ
        if reorganized_rows:
            total_product_ids = sum(len(row['product_ids']) for row in reorganized_rows)
            total_image_urls = sum(len(row['image_urls']) for row in reorganized_rows)
            total_page_urls = sum(len(row['page_urls']) for row in reorganized_rows)
            total_prices = sum(len(row['prices']) for row in reorganized_rows)
            
            print(f"  ç”¢å“ID: {total_product_ids} å€‹")
            print(f"  åœ–ç‰‡URL: {total_image_urls} å€‹")
            print(f"  é é¢URL: {total_page_urls} å€‹")
            print(f"  åƒ¹æ ¼: {total_prices} å€‹")
            
            avg_image_urls = total_image_urls / len(reorganized_rows)
            print(f"  å¹³å‡æ¯è¡Œåœ–ç‰‡URL: {avg_image_urls:.1f}")
        
        return reorganized_rows
        
    except Exception as e:
        print(f"âŒ é‡æ–°çµ„ç¹” {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

def analyze_row_data(all_data, row_index):
    """åˆ†æå–®è¡Œè³‡æ–™"""
    try:
        # æå– URL
        urls = []
        for data_piece in all_data:
            urls.extend(re.findall(r'https?://[^\s\x00]+', data_piece))
        
        # æå–ç”¢å“ ID
        product_ids = []
        for data_piece in all_data:
            product_ids.extend(re.findall(r'\d{10,}', data_piece))
        
        # æå–åƒ¹æ ¼
        prices = []
        for data_piece in all_data:
            prices.extend(re.findall(r'\d+\s*-\s*\d+', data_piece))
        
        # æå–å•†åº—åç¨±
        shops = []
        shop_patterns = [
            r'([a-zA-Z0-9_]+)ruten',
            r'([a-zA-Z0-9_]+)shopee',
            r'([a-zA-Z0-9_]+)momo'
        ]
        for data_piece in all_data:
            for pattern in shop_patterns:
                shops.extend(re.findall(pattern, data_piece))
        
        # åˆ†é¡ URL
        image_urls = [url for url in urls if 'rimg.com.tw' in url or 'img' in url]
        page_urls = [url for url in urls if 'goods.ruten.com.tw' in url or 'item/show' in url]
        
        # å»é‡
        product_ids = list(set(product_ids))
        image_urls = list(set(image_urls))
        page_urls = list(set(page_urls))
        prices = list(set(prices))
        shops = list(set(shops))
        
        # æ§‹å»ºçµæ§‹åŒ–è¨˜éŒ„
        record = {
            'row_index': row_index,
            'product_ids': product_ids,
            'image_urls': image_urls,
            'page_urls': page_urls,
            'prices': prices,
            'shops': shops,
            'other_data': extract_other_data(all_data)
        }
        
        return record
        
    except Exception as e:
        print(f"åˆ†æç¬¬ {row_index} è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

def extract_other_data(all_data):
    """æå–å…¶ä»–è³‡æ–™"""
    try:
        # åˆä½µæ‰€æœ‰è³‡æ–™
        combined = ' '.join(all_data)
        
        # ç§»é™¤å·²çŸ¥çš„è³‡æ–™é¡å‹
        cleaned = combined
        
        # ç§»é™¤ URL
        cleaned = re.sub(r'https?://[^\s\x00]+', '', cleaned)
        
        # ç§»é™¤ç”¢å“ ID
        cleaned = re.sub(r'\d{10,}', '', cleaned)
        
        # ç§»é™¤åƒ¹æ ¼
        cleaned = re.sub(r'\d+\s*-\s*\d+', '', cleaned)
        
        # ç§»é™¤å•†åº—åç¨±
        cleaned = re.sub(r'[a-zA-Z0-9_]+(ruten|shopee|momo)', '', cleaned)
        
        # æ¸…ç†ç©ºç™½å­—å…ƒ
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        
        # åªä¿ç•™æœ‰æ„ä¹‰çš„è³‡æ–™
        if len(cleaned) > 10:
            return cleaned[:200]  # é™åˆ¶é•·åº¦
        return ""
        
    except Exception as e:
        return ""

def save_reorganized_data(all_results):
    """ä¿å­˜é‡æ–°çµ„ç¹”çš„è³‡æ–™"""
    print(f"\nğŸ’¾ ä¿å­˜é‡æ–°çµ„ç¹”çš„è³‡æ–™")
    print("=" * 60)
    
    output_dir = Path("../data/correct_structure_data")
    output_dir.mkdir(exist_ok=True)
    
    for table_name, records in all_results.items():
        if not records:
            continue
            
        # è½‰æ›ç‚º DataFrame
        df_data = []
        
        for record in records:
            row = {
                'product_ids': json.dumps(record['product_ids'], ensure_ascii=False),
                'image_urls': json.dumps(record['image_urls'], ensure_ascii=False),
                'page_urls': json.dumps(record['page_urls'], ensure_ascii=False),
                'prices': json.dumps(record['prices'], ensure_ascii=False),
                'shops': json.dumps(record['shops'], ensure_ascii=False),
                'link': record['other_data']
            }
            df_data.append(row)
        
        if df_data:
            df = pd.DataFrame(df_data)
            
            # ä¿å­˜ç‚º CSV
            output_file = output_dir / f"{table_name.lower().replace(' ', '_')}_correct.csv"
            df.to_csv(output_file, index=False, encoding='utf-8-sig')
            
            print(f"âœ… å·²ä¿å­˜: {output_file}")
            print(f"   è¨˜éŒ„æ•¸: {len(df)}")
            print(f"   æ¬„ä½æ•¸: {len(df.columns)}")
            print(f"   æ¬„ä½: {list(df.columns)}")
            
            # é¡¯ç¤ºç¯„ä¾‹
            if len(df) > 0:
                print(f"   ç¯„ä¾‹ç”¢å“ID: {df.iloc[0]['product_ids'][:100]}...")
                print(f"   ç¯„ä¾‹åœ–ç‰‡URLæ•¸é‡: {len(json.loads(df.iloc[0]['image_urls']))}")

def main():
    """ä¸»å‡½æ•¸"""
    results = parse_real_structure()
    
    if results:
        print(f"\nğŸ“Š é‡æ–°çµ„ç¹”å®Œæˆç¸½çµ:")
        print("=" * 60)
        
        for table_name, records in results.items():
            if records:
                print(f"\n{table_name}:")
                print(f"  è¨˜éŒ„æ•¸: {len(records)}")
                
                avg_product_ids = sum(len(r['product_ids']) for r in records) / len(records)
                avg_image_urls = sum(len(r['image_urls']) for r in records) / len(records)
                avg_page_urls = sum(len(r['page_urls']) for r in records) / len(records)
                
                print(f"  å¹³å‡ç”¢å“IDæ•¸: {avg_product_ids:.1f}")
                print(f"  å¹³å‡åœ–ç‰‡URLæ•¸: {avg_image_urls:.1f}")
                print(f"  å¹³å‡é é¢URLæ•¸: {avg_page_urls:.1f}")
                
                print(f"  çœŸå¯¦æ¬„ä½çµæ§‹:")
                print(f"    1. product_ids - ç”¢å“è­˜åˆ¥ç¢¼åˆ—è¡¨")
                print(f"    2. image_urls - åœ–ç‰‡URLåˆ—è¡¨ ({avg_image_urls:.0f} å€‹)")
                print(f"    3. page_urls - ç”¢å“é é¢URLåˆ—è¡¨")
                print(f"    4. prices - åƒ¹æ ¼è³‡è¨Šåˆ—è¡¨")
                print(f"    5. shops - å•†åº—è³‡è¨Šåˆ—è¡¨")
                print(f"    6. link - å°æ‡‰çš„ROOT ID")

if __name__ == "__main__":
    main()
