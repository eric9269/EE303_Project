"""
å¿«é€Ÿé©—è­‰è…³æœ¬
å¿«é€Ÿæª¢æŸ¥æ‰€æœ‰å·¥å…·æ˜¯å¦å¯ç”¨
"""

import os
import sys
import subprocess
from pathlib import Path
import logging

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def quick_check():
    """å¿«é€Ÿæª¢æŸ¥æ‰€æœ‰å·¥å…·"""
    print("ğŸ” å¿«é€Ÿé©—è­‰æ‰€æœ‰å·¥å…·...")
    print("="*60)
    
    # æª¢æŸ¥ç›®éŒ„çµæ§‹
    directories = [
        "data_processing",
        "embedding_processors", 
        "dataset_generators",
        "model_training",
        "model_testing",
        "pipelines"
    ]
    
    print("ğŸ“ æª¢æŸ¥ç›®éŒ„çµæ§‹:")
    for dir_name in directories:
        if Path(dir_name).exists():
            print(f"  âœ… {dir_name}/")
        else:
            print(f"  âŒ {dir_name}/ (ç¼ºå¤±)")
    
    print("\nğŸ“„ æª¢æŸ¥ Python æª”æ¡ˆ:")
    
    # æª¢æŸ¥é—œéµæª”æ¡ˆ
    key_files = [
        "data_processing/simple_correct_parser.py",
        "data_processing/view_correct_data.py",
        "embedding_processors/clip_image_processor.py",
        "embedding_processors/text_embedding_processor.py",
        "dataset_generators/bm25_sampler.py",
        "dataset_generators/classification_dataset_generator.py",
        "dataset_generators/triplet_dataset_generator.py",
        "dataset_generators/generate_training_datasets.py",
        "model_training/train_triplet_model.py",
        "model_training/train_classification_model.py",
        "model_testing/test_models.py",
        "pipelines/train_and_test_pipeline.py"
    ]
    
    missing_files = []
    for file_path in key_files:
        if Path(file_path).exists():
            print(f"  âœ… {file_path}")
        else:
            print(f"  âŒ {file_path} (ç¼ºå¤±)")
            missing_files.append(file_path)
    
    print("\nğŸ”§ æª¢æŸ¥ help åŠŸèƒ½:")
    
    # æ¸¬è©¦ help åŠŸèƒ½
    help_test_files = [
        "data_processing/simple_correct_parser.py",
        "dataset_generators/bm25_sampler.py",
        "dataset_generators/classification_dataset_generator.py",
        "dataset_generators/triplet_dataset_generator.py",
        "model_training/train_triplet_model.py",
        "model_training/train_classification_model.py",
        "model_testing/test_models.py",
        "pipelines/train_and_test_pipeline.py"
    ]
    
    for file_path in help_test_files:
        if Path(file_path).exists():
            try:
                result = subprocess.run([
                    sys.executable, file_path, "--help"
                ], capture_output=True, text=True, timeout=5)
                
                if result.returncode == 0:
                    print(f"  âœ… {file_path} (help æ­£å¸¸)")
                else:
                    print(f"  âš ï¸ {file_path} (help æœ‰å•é¡Œ)")
            except Exception as e:
                print(f"  âŒ {file_path} (help å¤±æ•—: {e})")
        else:
            print(f"  âŒ {file_path} (æª”æ¡ˆä¸å­˜åœ¨)")
    
    print("\nğŸ“‹ æª¢æŸ¥ README æª”æ¡ˆ:")
    
    # æª¢æŸ¥ README æª”æ¡ˆ
    readme_files = [
        "README.md",
        "data_processing/README.md",
        "embedding_processors/README.md",
        "dataset_generators/README.md",
        "model_training/README.md",
        "model_testing/README.md",
        "pipelines/README.md"
    ]
    
    for readme_path in readme_files:
        if Path(readme_path).exists():
            print(f"  âœ… {readme_path}")
        else:
            print(f"  âŒ {readme_path} (ç¼ºå¤±)")
    
    print("\n" + "="*60)
    
    if missing_files:
        print(f"âš ï¸ ç™¼ç¾ {len(missing_files)} å€‹ç¼ºå¤±æª”æ¡ˆ")
        return False
    else:
        print("ğŸ‰ æ‰€æœ‰æª”æ¡ˆéƒ½å­˜åœ¨ï¼")
        return True

def check_dependencies():
    """æª¢æŸ¥ä¾è³´"""
    print("\nğŸ” æª¢æŸ¥ä¾è³´å¥—ä»¶...")
    print("="*60)
    
    dependencies = {
        'torch': 'PyTorch',
        'transformers': 'Transformers', 
        'clip': 'CLIP',
        'sklearn': 'Scikit-learn',
        'pandas': 'Pandas',
        'numpy': 'NumPy',
        'requests': 'Requests',
        'PIL': 'Pillow'
    }
    
    missing_deps = []
    available_deps = []
    
    for module, name in dependencies.items():
        try:
            __import__(module)
            available_deps.append(name)
            print(f"  âœ… {name}")
        except ImportError:
            missing_deps.append(name)
            print(f"  âŒ {name} (ç¼ºå¤±)")
    
    print("\n" + "="*60)
    
    if missing_deps:
        print(f"âš ï¸ ç¼ºå¤± {len(missing_deps)} å€‹ä¾è³´: {', '.join(missing_deps)}")
        print("\nå®‰è£æŒ‡ä»¤:")
        print("pip install torch transformers clip-by-openai scikit-learn pandas numpy requests pillow")
        return False
    else:
        print("ğŸ‰ æ‰€æœ‰ä¾è³´éƒ½å·²å®‰è£ï¼")
        return True

def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å¿«é€Ÿé©—è­‰å·¥å…·')
    parser.add_argument('--check_deps', action='store_true', help='æª¢æŸ¥ä¾è³´')
    parser.add_argument('--check_files', action='store_true', help='æª¢æŸ¥æª”æ¡ˆ')
    
    args = parser.parse_args()
    
    if args.check_deps:
        check_dependencies()
    elif args.check_files:
        quick_check()
    else:
        # åŸ·è¡Œæ‰€æœ‰æª¢æŸ¥
        files_ok = quick_check()
        deps_ok = check_dependencies()
        
        print("\nğŸ“Š ç¸½çµ:")
        print("="*60)
        if files_ok:
            print("ğŸ‰ æ‰€æœ‰æª”æ¡ˆæª¢æŸ¥éƒ½é€šéï¼å·¥å…·å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")
            if not deps_ok:
                print("âš ï¸ ç¼ºå°‘å¯é¸ä¾è³´ CLIPï¼Œä½†ä¸å½±éŸ¿åŸºæœ¬åŠŸèƒ½ã€‚")
            sys.exit(0)
        else:
            print("âš ï¸ ç™¼ç¾å•é¡Œï¼Œè«‹æª¢æŸ¥ä¸Šè¿°éŒ¯èª¤è¨Šæ¯ã€‚")
            sys.exit(1)

if __name__ == "__main__":
    main()
